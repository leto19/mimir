{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification\n",
    "\n",
    "**Aim**: Establish baseline(s) and subsequent models for question classification using the Trec(6) dataset.\n",
    "\n",
    "It would also be good to test these models on a small test set of labelled questions from narrativeQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports\n",
    "'''\n",
    "\n",
    "import os, re, string, sys\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['DESC', 'LOC', 'HUM', 'ENTY', 'ABBR', 'NUM']\n",
      "Classes counts:\n",
      "- DESC: 1162\n",
      "- LOC: 835\n",
      "- HUM: 1223\n",
      "- ENTY: 1250\n",
      "- ABBR: 86\n",
      "- NUM: 896\n",
      "{'DESC': 0, 'LOC': 1, 'HUM': 2, 'ENTY': 3, 'ABBR': 4, 'NUM': 5}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load/parse dataset\n",
    "'''\n",
    "\n",
    "xtrain = []\n",
    "ytrain =[]\n",
    "xtest = []\n",
    "ytest = []    \n",
    "\n",
    "# Train\n",
    "with open(\"../../data/question_classification/trec_train.txt\", 'rb') as f:\n",
    "    questions = [x.decode('utf8').strip() for x in f.readlines()]\n",
    "    for q in questions:\n",
    "        splt = q.replace(\"\\n\", \"\").split(\":\")\n",
    "        ytrain.append(splt[0])\n",
    "        xtrain.append(\" \".join(splt[1].split(\" \")[1:]))\n",
    "        \n",
    "# Test\n",
    "with open(\"../../data/question_classification/trec_test.txt\", 'rb') as f:\n",
    "    questions = [x.decode('utf8').strip() for x in f.readlines()]\n",
    "    for q in questions:\n",
    "        splt = q.replace(\"\\n\", \"\").split(\":\")\n",
    "        ytest.append(splt[0])\n",
    "        xtest.append(\" \".join(splt[1].split(\" \")[1:]))\n",
    "        \n",
    "classes = dict(enumerate(list(set(ytrain))))\n",
    "reverse_classes = {v: k for k, v in classes.items()}\n",
    "\n",
    "print(\"Classes: \", list(reverse_classes.keys()))\n",
    "print(\"Classes counts:\")\n",
    "for cls in list(reverse_classes.keys()):\n",
    "    print(\"- {}: {}\".format(cls, len([x for x in ytrain if x == cls])))\n",
    "    \n",
    "print(reverse_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances:  5452\n",
      "Testing instances:  500\n",
      "\n",
      "Training examples:\n",
      "['How did serfdom develop in and then leave Russia', 'What films featured the character Popeye Doyle', 'How can I find a list of celebrities real names', 'What fowl grabs the spotlight after the Chinese Year of the Monkey', 'What is the full form of com']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preprocess data\n",
    "'''\n",
    "\n",
    "def preprocess_text(data):\n",
    "    # Remove punctuation\n",
    "    exclude = set(string.punctuation)\n",
    "    data = [''.join(ch for ch in x if ch not in exclude).strip() for x in data]\n",
    "    \n",
    "    # Remove multi-spaces\n",
    "    data = [re.sub(' +', ' ', x) for x in data]\n",
    "    return data\n",
    "\n",
    "xtrain = preprocess_text(xtrain)\n",
    "xtest = preprocess_text(xtest)\n",
    "\n",
    "print(\"Training instances: \", len(xtrain))\n",
    "print(\"Testing instances: \", len(xtest))\n",
    "print(\"\\nTraining examples:\")\n",
    "print(xtrain[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class HUM question examples: \n",
      "\n",
      "What contemptible scoundrel stole the cork from my lunch\n",
      "What team did baseball s St Louis Browns become\n",
      "What is the oldest profession\n",
      "Name the scarfaced bounty hunter of The Old West\n",
      "Who was The Pride of the Yankees\n",
      "Who killed Gandhi\n",
      "Name 11 famous martyrs\n",
      "Who was the inventor of silly putty\n",
      "Which company that manufactures videogame hardware sells the super system\n",
      "What 1920s cowboy star rode Tony the Wonder Horse\n",
      "----------------------------------------\n",
      "Class DESC question examples: \n",
      "\n",
      "How did serfdom develop in and then leave Russia\n",
      "How can I find a list of celebrities real names\n",
      "What are liver enzymes\n",
      "Why do heavier objects travel downhill faster\n",
      "What did the only repealed amendment to the US Constitution deal with\n",
      "What is Nine Inch Nails\n",
      "What is an annotated bibliography\n",
      "What s the Olympic motto\n",
      "What is the origin of the name Scarlett\n",
      "What do Mormons believe\n",
      "----------------------------------------\n",
      "Class LOC question examples: \n",
      "\n",
      "What sprawling US state boasts the most airports\n",
      "What is the highest waterfall in the United States\n",
      "Which two states enclose Chesapeake Bay\n",
      "Where do the adventures of The Swiss Family Robinson take place\n",
      "What country do the Galapagos Islands belong to\n",
      "What US state lived under six flags\n",
      "Where is the Loop\n",
      "What country s capital is Tirana\n",
      "Which city has the oldest relationship as a sister city with Los Angeles\n",
      "What are the names of the tourist attractions in Reims\n",
      "----------------------------------------\n",
      "Class ENTY question examples: \n",
      "\n",
      "What films featured the character Popeye Doyle\n",
      "What fowl grabs the spotlight after the Chinese Year of the Monkey\n",
      "What is considered the costliest disaster the insurance industry has ever faced\n",
      "What articles of clothing are tokens in Monopoly\n",
      "What s the secondmostused vowel in English\n",
      "Name a golf course in Myrtle Beach\n",
      "What does a spermologer collect\n",
      "In what religion was Isis the nature goddess\n",
      "What relative of the racoon is sometimes known as the catbear\n",
      "What was cashconscious Colonel Edwin L Drake the first to drill\n",
      "----------------------------------------\n",
      "Class ABBR question examples: \n",
      "\n",
      "What is the full form of com\n",
      "What does the abbreviation AIDS stand for\n",
      "What does INRI stand for when used on Jesus cross\n",
      "What does SOS stand for\n",
      "CNN is the abbreviation for what\n",
      "What does PSI stand for\n",
      "What is the abbreviation for micro\n",
      "What does NASDAQ stand for\n",
      "What is BPH\n",
      "What is the abbreviation of the company name General Motors\n",
      "----------------------------------------\n",
      "Class NUM question examples: \n",
      "\n",
      "When was Ozzy Osbourne born\n",
      "How many Jews were executed in concentration camps during WWII\n",
      "What is the date of Boxing Day\n",
      "How many points make up a perfect fivepin bowling score\n",
      "How many Community Chest cards are there in Monopoly\n",
      "When did the neanderthal man live\n",
      "How many people in the world speak French\n",
      "How many inches over six feet is the Venus de Milo\n",
      "How many species of the Great White shark are there\n",
      "How many villi are found in the small intestine\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Inspect data\n",
    "'''\n",
    "\n",
    "for cls in reverse_classes.keys():\n",
    "    print(\"Class {} question examples: \\n\".format(cls))\n",
    "    idxs = [i for i, x in enumerate(ytrain) if x == cls]\n",
    "    for ex in np.array(xtrain)[idxs][:10]:\n",
    "        print(\"{}\".format(ex))\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Util functions\n",
    "'''\n",
    "\n",
    "def plot_confusion_matrix(test_predictions):\n",
    "    cm = confusion_matrix(ytest, test_predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Baseline 1 - Question words\n",
    "\n",
    "Create and test a simple baseline that makes use of question words to classify questions into one of the 6 question classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Establish baseline 1\n",
    "'''\n",
    "        \n",
    "def get_named_entities(tokens):\n",
    "    entities = nltk.chunk.ne_chunk(nltk.pos_tag(tokens))\n",
    "    return entities\n",
    "\n",
    "# Jonathan's baseline adapted\n",
    "class SimpleBaseline:\n",
    "    def __init__(self):\n",
    "        self.categories = classes.keys()\n",
    "                            \n",
    "    def classify_question(self, question):\n",
    "        question_tokens = word_tokenize(question) \n",
    "        words_and_labels = get_named_entities(question_tokens)\n",
    "        entities = [ne for ne in words_and_labels if isinstance(ne, nltk.tree.Tree)]\n",
    "\n",
    "        labels = [e.label() for e in entities]\n",
    "\n",
    "        # HUM\n",
    "        who_condition = \"who\" in [t.lower() for t in question_tokens]\n",
    "        whose_condition = \"whose\" in [t.lower() for t in question_tokens]\n",
    "        whos_condition = \"who's\" in [t.lower() for t in question_tokens]\n",
    "        whom_condition = \"whom\" in [t.lower() for t in question_tokens]\n",
    "        \n",
    "        which_condition = \"which\" in [t.lower() for t in question_tokens]\n",
    "\n",
    "\n",
    "        # ENTY\n",
    "        what_condition = \"what\" in [t.lower() for t in question_tokens]\n",
    "        \n",
    "        # LOC\n",
    "        where_condition = \"where\" in [t.lower() for t in question_tokens]\n",
    "        \n",
    "        # DESC\n",
    "        when_condition = \"when\" in [t.lower() for t in question_tokens]\n",
    "        why_condition = \"why\" in [t.lower() for t in question_tokens]\n",
    "        how_condition = \"how\" in [t.lower() for t in question_tokens]\n",
    "        \n",
    "        was_condition = \"was\" in [t.lower() for t in question_tokens]\n",
    "        did_condition = \"did\" in [t.lower() for t in question_tokens]\n",
    "        \n",
    "        named_person_condition = \"PERSON\" in labels\n",
    "\n",
    "        \n",
    "        if who_condition or whose_condition or whos_condition or whom_condition or (which_condition and named_person_condition) or [t.lower() for t in question_tokens[:1]] == [\"name\"]:\n",
    "            return(\"HUM\") #We want a person's name\n",
    "        \n",
    "                \n",
    "        if when_condition or [t.lower() for t in question_tokens[:2]] == [\"how\", \"many\"] or (when_condition and (was_condition or did_condition)) or (what_condition and 'date' in [t.lower() for t in question_tokens]):\n",
    "            return(\"NUM\") #Number\n",
    "\n",
    "        \n",
    "        if what_condition and \"stand\" in [t.lower() for t in question_tokens] and \"for\" in [t.lower() for t in question_tokens] or \"abbreviation\" in [t.lower() for t in question_tokens] :\n",
    "            return(\"ABBR\")\n",
    "        \n",
    "                \n",
    "        if where_condition or (what_condition and (\"country\" in [t.lower() for t in question_tokens] or \"state\" in [t.lower() for t in question_tokens])):\n",
    "            return(\"LOC\") #Location\n",
    "        \n",
    "        if [t.lower() for t in question_tokens[:2]] == [\"what\", \"is\"]:\n",
    "            return(\"DESC\")\n",
    "        \n",
    "        if (what_condition and not named_person_condition) or which_condition:\n",
    "            return(\"ENTY\")\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            return(\"DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test baseline 1 (does not need training)\n",
    "'''\n",
    "\n",
    "SB = SimpleBaseline()\n",
    "\n",
    "predictions_b1 = []\n",
    "\n",
    "bs_x = xtest\n",
    "bs_y = ytest\n",
    "\n",
    "for q in bs_x:\n",
    "    pred_cls = SB.classify_question(q)\n",
    "    predictions_b1.append(pred_cls)\n",
    "\n",
    "print(classification_report(bs_y, predictions_b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(classes[i])\n",
    "print(len([x for x in predictions_b1 if x == classes[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(predictions_b1) # The labels for this are currently wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Baseline 2 - NLU package\n",
    "\n",
    "Import and test a pre-trained question classifier from the Natural Language Understanding library and a more advanced baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlu_cls = nlu.load('en.classify.trec6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_b2 = []\n",
    "\n",
    "# bs_x = xtest\n",
    "# bs_y = ytest\n",
    "\n",
    "# for q in bs_x:\n",
    "#     pred_cls = nlu_cls.predict(q)\n",
    "#     predictions_b2.append(pred_cls)\n",
    "    \n",
    "# print(classification_report(bs_y, predictions_b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## BOW Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if str(word) not in stop_words and str(word) not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(max_iter = 2000)\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc = [reverse_classes[x] for x in ytrain]\n",
    "ytest_enc = [reverse_classes[x] for x in ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(xtrain, ytrain_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipe.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bs_y, [classes[y] for y in predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Sentence Embedding Logistic Regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.nlp(text).vector for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "column_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('text_glove', GloveVectorTransformer(nlp), 'text'),\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Create pipeline using sentence embedding \n",
    "pipe2 = Pipeline([('column_preprocessor', column_preprocessor),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df = pd.DataFrame(np.array(xtrain))\n",
    "xtrain_df.columns = [\"text\"]\n",
    "pipe2.fit(xtrain_df, ytrain_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_df = pd.DataFrame(np.array(xtest))\n",
    "xtest_df.columns = [\"text\"]\n",
    "predicted = pipe2.predict(xtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bs_y, [classes[y] for y in predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## InferSent model - Log Reg \n",
    "\n",
    "Inspired by SOTA on paperswithcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pre-trained encoder\n",
    "from models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent.build_vocab(xtrain, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_train = infersent.encode(xtrain, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(inf_train, ytrain_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_test = infersent.encode(xtest, tokenize=True)\n",
    "predicted = classifier.predict(inf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bs_y, [classes[y] for y in predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## InferSent model - CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding size = 300 \n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.outsize = 10\n",
    "        \n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv_1 = nn.Conv1d(1, self.outsize, self.kernel_1, 1) \n",
    "        self.conv_2 = nn.Conv1d(1, self.outsize, self.kernel_2, 1)\n",
    "        self.conv_3 = nn.Conv1d(1, self.outsize, self.kernel_3, 1)\n",
    "        self.conv_4 = nn.Conv1d(1, self.outsize, self.kernel_4, 1)\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4)\n",
    "        self.fc = nn.Linear(52520, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution layer 1 is applied\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = F.leaky_relu(x1, negative_slope=0.03)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        # Convolution layer 2 is applied\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = F.leaky_relu(x2, negative_slope=0.03)\n",
    "        x2 = self.pool_2(x2)\n",
    "\n",
    "        # Convolution layer 3 is applied\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = F.leaky_relu(x3, negative_slope=0.03)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        # Convolution layer 4 is applied\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = F.leaky_relu(x4, negative_slope=0.03)\n",
    "        x4 = self.pool_4(x4)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "                \n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.fc(union)\n",
    "        \n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "cnn_epochs = 20\n",
    "cnn_lr = 0.01\n",
    "cnn_lambda = 0.005\n",
    "batchSize = 50\n",
    "\n",
    "# Instatiate model\n",
    "myCNN = CNN()\n",
    "optimizer = optim.SGD(myCNN.parameters(), lr=cnn_lr, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = [(v, ytrain_enc[i]) for i, v in enumerate(inf_train)]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(load_data, batch_size=batchSize, shuffle=True, \n",
    "                                          num_workers=2, worker_init_fn=np.random.seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0 \n",
    "\n",
    "for epoch in range(cnn_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = criterion(myCNN(inputs), labels)\n",
    "        loss = output.item()\n",
    "\n",
    "        # Backward pass\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss\n",
    "        if i % 200 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # Stop criterion\n",
    "        if abs(loss) < 1e-2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(inf_test, batch_size=batchSize, shuffle=True, \n",
    "                                          num_workers=2, worker_init_fn=np.random.seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs = data.unsqueeze(1)\n",
    "        outputs = myCNN(inputs)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        print(pred)\n",
    "        predicted += [x.item() for x in list(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bs_y, [classes[y] for y in predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Infersent RNN with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8990(/9281) words with w2v vectors\n",
      "Vocab size : 8990\n"
     ]
    }
   ],
   "source": [
    "# Load in pre-trained encoder\n",
    "from models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)\n",
    "\n",
    "infersent.build_vocab(xtrain, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "ytrain_enc = [reverse_classes[x] for x in ytrain]\n",
    "ytest_enc = [reverse_classes[x] for x in ytest]\n",
    "inf_train = infersent.encode(xtrain, tokenize=True) \n",
    "inf_test = infersent.encode(xtest, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "num_train_examples = len(inf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inf_train = np.array([x.reshape(1, 4096) for x in inf_train])\n",
    "inf_test = np.array([x.reshape(1, 4096) for x in inf_test])\n",
    "\n",
    "tf_train = tf.convert_to_tensor(np.array(inf_train))\n",
    "tf_test = tf.convert_to_tensor(np.array(inf_test))\n",
    "tf_y = tf.convert_to_tensor(np.array(ytrain_enc))\n",
    "tf_test_y = tf.convert_to_tensor(np.array(ytest_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_data = tf.data.Dataset.from_tensor_slices((tf_train, tf_y))\n",
    "tf_test_data = tf.data.Dataset.from_tensor_slices((tf_test, tf_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf_train_data.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = tf_test_data.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4096, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(6)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 191s 3s/step - loss: 1.3383 - accuracy: 0.5100\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 192s 3s/step - loss: 0.8179 - accuracy: 0.7211\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 191s 3s/step - loss: 0.5995 - accuracy: 0.7958\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 191s 3s/step - loss: 0.4566 - accuracy: 0.8471\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 192s 3s/step - loss: 0.3756 - accuracy: 0.8760\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 201s 4s/step - loss: 0.3217 - accuracy: 0.8907\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 193s 4s/step - loss: 0.2787 - accuracy: 0.9091\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 194s 4s/step - loss: 0.2314 - accuracy: 0.9233\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 193s 4s/step - loss: 0.2069 - accuracy: 0.9364\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 193s 4s/step - loss: 0.1867 - accuracy: 0.9435\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 535ms/step - loss: 0.3035 - accuracy: 0.9040\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 194s 4s/step - loss: 0.1611 - accuracy: 0.9522\n",
      "5/5 [==============================] - 3s 543ms/step - loss: 0.2964 - accuracy: 0.9080\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=1, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 192s 3s/step - loss: 0.1322 - accuracy: 0.9627\n",
      "5/5 [==============================] - 3s 534ms/step - loss: 0.2692 - accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=1, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomasg/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./rnn_model/assets\n",
      "assets\tsaved_model.pb\tvariables\n"
     ]
    }
   ],
   "source": [
    "export_path_sm = \"./{}\".format(\"rnn_model\")\n",
    "tf.saved_model.save(model, export_path_sm)\n",
    "!ls {export_path_sm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: rnn_model/ (stored 0%)\n",
      "  adding: rnn_model/variables/ (stored 0%)\n",
      "  adding: rnn_model/variables/variables.data-00000-of-00001 (deflated 46%)\n",
      "  adding: rnn_model/variables/variables.index (deflated 68%)\n",
      "  adding: rnn_model/saved_model.pb (deflated 91%)\n",
      "  adding: rnn_model/assets/ (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model.zip {export_path_sm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  .download('./model.zip')\n",
    "except ImportError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=1, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Infersent CNN with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_train_cnn = np.array([x.reshape(4096, 1) for x in inf_train])\n",
    "inf_test_cnn = np.array([x.reshape(4096, 1) for x in inf_test])\n",
    "\n",
    "tf_train_cnn = tf.convert_to_tensor(np.array(inf_train_cnn))\n",
    "tf_test_cnn = tf.convert_to_tensor(np.array(inf_test_cnn))\n",
    "\n",
    "tf_train_data = tf.data.Dataset.from_tensor_slices((tf_train_cnn, tf_y))\n",
    "tf_test_data = tf.data.Dataset.from_tensor_slices((tf_test_cnn, tf_test_y))\n",
    "\n",
    "train_dataset = tf_train_data.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = tf_test_data.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(32, 4, activation='relu', input_shape = (4096, 1)),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Conv1D(32, 4, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(6)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
