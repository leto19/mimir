{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install termcolor\n",
    "!pip install transformers\n",
    "!pip install pycocoevalcap\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "!spacy download en_core_web_sm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using CPU instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import os.path as op\n",
    "import qa.corpus_utils.regex_utils\n",
    "from qa.question_answering.models.model import active_models\n",
    "from qa.question_answering.utils import mimir_dir, data_dir, csv_to_list, tokenize, make_id_name_dict, make_qa_dict_valid\n",
    "from qa.evaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name_dict = make_id_name_dict()  # Dictionary of book IDs by name\n",
    "qaps_line_list = csv_to_list(op.join(data_dir, \"narrativeqa_qas.csv\"))\n",
    "qa_dict_valid = make_qa_dict_valid(qaps_line_list, id_name_dict)  # Questions and answers from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cosine_distance_bow_fulltext', 'bert_baseline', 'distilbert_squad_nqa', 'attribute_model_fulltext', 'character_list_model']\n"
     ]
    }
   ],
   "source": [
    "models_list = list(active_models.keys())\n",
    "print(models_list)\n",
    "qa_dict_debug = {k: v for k, v in list(qa_dict_valid.items())[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are initializing bert baseline\n",
      "bert model\n",
      "bert model to device\n",
      "self.tokenizer\n",
      "initialized\n",
      "we are initializing distilbert finetuned on SQuAD and NQA\n",
      "initialized\n",
      "we are initializing bert baseline\n",
      "we are initializing bert baseline\n",
      "evaluating q 1 of 1725\n",
      "evaluating q 2 of 1725\n",
      "evaluating q 3 of 1725\n",
      "evaluating q 4 of 1725\n",
      "evaluating q 5 of 1725\n",
      "evaluating q 6 of 1725\n",
      "evaluating q 7 of 1725\n",
      "evaluating q 8 of 1725\n",
      "evaluating q 9 of 1725\n",
      "evaluating q 10 of 1725\n",
      "evaluating q 20 of 1725\n",
      "evaluating q 30 of 1725\n",
      "evaluating q 40 of 1725\n",
      "evaluating q 50 of 1725\n",
      "evaluating q 60 of 1725\n",
      "evaluating q 70 of 1725\n",
      "evaluating q 80 of 1725\n",
      "evaluating q 90 of 1725\n",
      "evaluating q 100 of 1725\n",
      "evaluating q 200 of 1725\n",
      "evaluating q 300 of 1725\n",
      "evaluating q 400 of 1725\n",
      "evaluating q 500 of 1725\n",
      "evaluating q 600 of 1725\n",
      "evaluating q 700 of 1725\n",
      "evaluating q 800 of 1725\n",
      "evaluating q 900 of 1725\n",
      "evaluating q 1000 of 1725\n",
      "evaluating q 1100 of 1725\n",
      "evaluating q 1200 of 1725\n",
      "evaluating q 1300 of 1725\n",
      "evaluating q 1400 of 1725\n",
      "evaluating q 1500 of 1725\n",
      "evaluating q 1600 of 1725\n",
      "evaluating q 1700 of 1725\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "\n\t\tAll arguments passed to similarity_metrics must be\t\n\t\tlists of untokenized sentences, e.g. \n\t\t\t[\"The cat sat on the mat\", \n\t\t\t \"The man ate the hotdog\", ...]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ccb6ed3f5d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_dict_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mextracted_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer_all_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bleu4_score, bleu1_score, rouge_score, cider_score, accuracy_score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MiniProject/mimir/qa/evaluate.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, extracted_answers)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mbleu4_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcider_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu4_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcider_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MiniProject/mimir/qa/evaluate.py\u001b[0m in \u001b[0;36msimilarity_metrics\u001b[0;34m(ref1_strs, ref2_strs, extracted_answers)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mlists\u001b[0m \u001b[0mof\u001b[0m \u001b[0muntokenized\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \t\t\t[\"The cat sat on the mat\", \n\u001b[0;32m---> 30\u001b[0;31m \t\t\t \"The man ate the hotdog\", ...]\"\"\")\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmeteor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeteor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: \n\t\tAll arguments passed to similarity_metrics must be\t\n\t\tlists of untokenized sentences, e.g. \n\t\t\t[\"The cat sat on the mat\", \n\t\t\t \"The man ate the hotdog\", ...]"
     ]
    }
   ],
   "source": [
    "#valid_eval = Evaluator(qa_dict_debug, models_list) # for debugging\n",
    "valid_eval = Evaluator(qa_dict_valid, models_list)\n",
    "extracted_answers = valid_eval.answer_all_questions(models_list[3])\n",
    "scores = valid_eval.score(extracted_answers)\n",
    "print(extracted_answers[:10])\n",
    "print(\"bleu4_score, bleu1_score, rouge_score, cider_score, accuracy_score\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
