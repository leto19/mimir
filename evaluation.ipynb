{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSC0wVWbde8R"
   },
   "source": [
    "# Dependencies and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oXFLDOtmbaG0"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install transformers\n",
    "#!pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "#!module load apps/java/jdk1.8.0_102/binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "z9SWHsZFb6mP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as op\n",
    "import time\n",
    "import datetime\n",
    "from string import punctuation\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def clean(text):\n",
    "  '''\n",
    "  Takes a string, removes leading and trailing whitespace,\n",
    "  makes it lower case, and removes leading and trailing punctuation.\n",
    "  '''\n",
    "  text = text.strip() # remove leading and trailing whitespace\n",
    "  text = text.lower() # lower case\n",
    "  text = text.strip(punctuation)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF3o2uw1dpwt"
   },
   "source": [
    "# Data Pre-processing\n",
    "In this section, we load the data required for training the model and perform any appropriate filtering/pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dJBqfwr4bAHZ",
    "outputId": "783893ed-e552-4723-f126-ebe27042a910"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "ROOT        = './'\n",
    "DATA_DIR    = ROOT + 'data/' \n",
    "MODELS_DIR  = ROOT + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "MF2jgHfAbO21"
   },
   "outputs": [],
   "source": [
    "#Load all data\n",
    "#Load all data\n",
    "qaps = pd.read_csv(op.join(DATA_DIR,'narrativeqa_qas.csv'))\n",
    "#qaps_2 = pd.read_csv(op.join(DATA_DIR,'narrativeqa_qaps_single_answer_2.csv'))\n",
    "\n",
    "summaries = pd.read_csv(DATA_DIR + 'summaries.csv')\n",
    "qaps = qaps[qaps['set']=='valid']\n",
    "summaries = summaries[summaries['set']=='valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "I5sp8-EAErHR"
   },
   "outputs": [],
   "source": [
    "summaries = summaries.set_index('document_id')\n",
    "summaries = summaries.drop(labels=['set','summary'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "FLu_De1S0y4q"
   },
   "outputs": [],
   "source": [
    "qaps = qaps.set_index('document_id')\n",
    "qaps = qaps.drop(labels=['set','question','answer1','answer2'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "8DemWvy7GKi3"
   },
   "outputs": [],
   "source": [
    "# pair qaps with their relevant summaries and drop non-tokenized fields\n",
    "qaps = qaps.join(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKc2Rc8Vg7Uf"
   },
   "source": [
    "Acquire data in lists of: contexts (summaries), questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "bTHQ7ufUbYlB"
   },
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "  for index, row in data.iterrows():\n",
    "    context   = row['summary_tokenized']\n",
    "    question  = row['question_tokenized']\n",
    "    answer    = {}\n",
    "    answer['answer1'] = clean(row['answer1_tokenized'])\n",
    "    answer['answer2'] = clean(row['answer2_tokenized'])\n",
    "\n",
    "    contexts.append(context)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "  \n",
    "  return contexts, questions, answers\n",
    "\n",
    "contexts, questions, answers = format_data(qaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "Eq90La2z9ktM"
   },
   "outputs": [],
   "source": [
    "# IF YOU WANT TO RUN TESTS WITH A SMALLER DATASET, UNCOMMENT THE CODE BELOW (ctrl + /)\n",
    "\n",
    "# questions = ['How big is the Empire State Building?', \n",
    "#                    'Who is Shrek married to?',\n",
    "#                    'How old is Gandalf?',\n",
    "#                    'Where does Winnie the Pooh live?']\n",
    "\n",
    "# contexts = ['The Empire State building is a very big building. It is one of the biggest buildings in the world. It is large.', \n",
    "#                   'Shrek is an ogre. There is a common misconception that Shrek is married to Donkey, but he is actually married to Fiona.',\n",
    "#                   'Gandalf is a 900 year old wizard.',\n",
    "#                   'Winnie the Pooh lives in Dalston with some of his uni housemates']\n",
    "\n",
    "# answers = [{'answer1': 'really big', 'answer2': 'very big'},\n",
    "#                  {'answer1': 'Shrek is married to Fiona', 'answer2': 'Fiona'},\n",
    "#                  {'answer1': 'very old', 'answer2': '900 years old'},\n",
    "#                  {'answer1': 'in London', 'answer2': 'near Dalston'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7fOz4-9dyoH"
   },
   "source": [
    "# Model information / hyperparameter selection\n",
    "In this section we have details on our model type, and the start-points for training (either a pre-trained model or a partially trained model we wish to resume training)\n",
    "\n",
    "To keep our experiments valid - ensure that the model id is of the form \"modelname-learning-rate\". \n",
    "\n",
    "If you resume training for a model, ensure the learning rates are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "AS1qyUVLd5IA"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, DistilBertTokenizerFast, DistilBertForQuestionAnswering, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "MODEL_IDS = ['distilbert-base-uncased-distilled-squad',\n",
    "             'distilbert-base-cased-distilled-squad',\n",
    "             'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "             'mrm8488/longformer-base-4096-finetuned-squadv2',\n",
    "             'distilbert-squad-nqa-5e-5',\n",
    "             'distilbert-squad-nqa-3e-5',\n",
    "             'bert-large-squad-nqa-3e-5'\n",
    "             'bert-large-squad-nqa-5e-5',\n",
    "             'bert-large-squad-nqa-5e-6',\n",
    "             'Primer/bart-squad2']\n",
    "\n",
    "model_types = ['distilbert', 'bert-base', 'bert-large', 'longformer']\n",
    "\n",
    "# select model id\n",
    "MODEL_ID      = MODEL_IDS[0]\n",
    "model_type    = model_types[0]\n",
    "finetuned     = False            # is the model saved on Drive\n",
    "epoch_to_load = 3               # if a model uploaded to Drive, which epoch to load\n",
    "\n",
    "batch_sizes = {'distilbert': 32, 'bert-base': 32, 'bert-large': 4, 'longformer': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-Zm8zlioWLv",
    "outputId": "67f24e2a-3c11-44a4-92ef-d169c8497992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-distilled-squad\n"
     ]
    }
   ],
   "source": [
    "#If we are training from scratch then load up the appropriate model, else load the partially trained model\n",
    "if finetuned:\n",
    "  MODEL_PATH = MODELS_DIR + MODEL_ID\n",
    "  MODEL_PATH = MODEL_PATH + '/epoch' + str(epoch_to_load)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "  model     = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH,\n",
    "                                                          output_attentions = False,\n",
    "                                                          output_hidden_states=False)\n",
    "  with open(MODEL_PATH + '/stats', \"rb\") as stats:\n",
    "    training_stats = pickle.load(stats)\n",
    "  print('Evaluating {} (finetuned for {} epochs)'.format(MODEL_ID, str(epoch_to_load)))\n",
    "\n",
    "else:\n",
    "  MODEL_PATH = MODEL_ID\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "  model     = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH,\n",
    "                                                          output_attentions = False,\n",
    "                                                          output_hidden_states=False)\n",
    "  print('Evaluating {}'.format(MODEL_ID))\n",
    "\n",
    "\n",
    "batch_size = batch_sizes[model_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lnnoUFndukz"
   },
   "source": [
    "# Data tokenization\n",
    "In this section we format our data so it is of the form required for GPU training.\n",
    "\n",
    "Additional features are (likely to be) added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "bZMNbB2JjuUs"
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(questions, contexts, truncation='only_second', padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "kUxfbrGKmHE1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "dataset = NQADataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X75_PxgNfu1"
   },
   "source": [
    "# Get Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "lWcZoUuZM7nH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "loader = DataLoader(\n",
    "            dataset, # The dev samples.\n",
    "            sampler = SequentialSampler(dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size) # Evaluate with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24NXcRg8NtBv",
    "outputId": "1aae3e9e-c5ae-4f85-895d-c58b538d596d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/acp20jac/.conda/example-python-env/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting answers positions...\n",
      "  Batch     1  of    109.    Elapsed: 0:00:22.\n",
      "  Batch     2  of    109.    Elapsed: 0:00:44.\n",
      "  Batch     3  of    109.    Elapsed: 0:01:06.\n",
      "  Batch     4  of    109.    Elapsed: 0:01:27.\n",
      "  Batch     5  of    109.    Elapsed: 0:01:49.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available() : model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Getting answers positions...\")\n",
    "\n",
    "total_val_loss = 0\n",
    "t0 = time.time()\n",
    "\n",
    "answer_starts = []\n",
    "answer_ends   = []\n",
    "answer_tokens = []\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for step, batch in enumerate(loader):\n",
    "\n",
    "    # Progress update every 40 batches.\n",
    "    if (step <= 5 or step % 40 == 0) and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "        # Report progress.\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(loader), elapsed))\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    # Don't construct a compute graph (only required for backprop during training)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    for i,score in enumerate(start_scores):\n",
    "      answer_start = torch.argmax(score)\n",
    "      answer_end = answer_start + torch.argmax(end_scores[i][answer_start:])\n",
    "      answer_starts.append(int(answer_start))\n",
    "      answer_ends.append(int(answer_end))\n",
    "      answer_tokens.append(input_ids[i].tolist())\n",
    "\n",
    "# Measure how long the dev run took.\n",
    "dev_time = format_time(time.time() - t0)    \n",
    "print(\"Answering took: {:}\".format(dev_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZULkkAGcR97v",
    "outputId": "cd505db6-4cb3-4320-cd15-36348aa8b218"
   },
   "outputs": [],
   "source": [
    "# Get answers using positions\n",
    "print(\"Getting answers...\")\n",
    "\n",
    "for i,(source,answer_start,answer_end) in enumerate(zip(answer_tokens, answer_starts,answer_ends)):\n",
    "  answer = tokenizer.decode(source[answer_start:answer_end+1])\n",
    "  answers[i]['extracted_answer'] = clean(answer)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Extracted answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATrrWACwyy6U",
    "outputId": "3340b326-63d7-4c02-9977-4705aa968ccf"
   },
   "outputs": [],
   "source": [
    "for answer in answers[:5]:\n",
    "  print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuBLd7F5A0mx"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1K9Qvxc4BkbA"
   },
   "outputs": [],
   "source": [
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "\n",
    "meteor_obj = Meteor()\n",
    "rouge_obj = Rouge()\n",
    "cider_obj = Cider()\n",
    "bleu_obj = Bleu(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "N59hqiELDdbN"
   },
   "outputs": [],
   "source": [
    "ref1_strs = [answer['answer1'] for answer in answers]\n",
    "ref2_strs = [answer['answer2'] for answer in answers]\n",
    "sys_strs  = [answer['extracted_answer'] for answer in answers]\n",
    "\n",
    "assert len(ref1_strs) == len(ref2_strs)\n",
    "assert len(ref2_strs) == len(sys_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dU8A1T3PFFR4",
    "outputId": "d9e17c9b-1f37-4916-b840-6ee547eec11d"
   },
   "outputs": [],
   "source": [
    "word_target_dict = {}\n",
    "word_response_dict = {}\n",
    "\n",
    "for i in range(len(ref1_strs)):\n",
    "    word_target_dict[i] = [ref1_strs[i], ref2_strs[i]]\n",
    "    word_response_dict[i] = [sys_strs[i]]\n",
    "\n",
    "\n",
    "bleu_score, bleu_scores = bleu_obj.compute_score(\n",
    "        word_target_dict, word_response_dict,\n",
    "        verbose=False)\n",
    "bleu1_score, _, _, bleu4_score = bleu_score\n",
    "bleu1_scores, _, _, bleu4_scores = bleu_scores\n",
    "meteor_score, meteor_scores = meteor_obj.compute_score(\n",
    "        word_target_dict, word_response_dict) \n",
    "rouge_score, rouge_scores = rouge_obj.compute_score(\n",
    "        word_target_dict, word_response_dict) \n",
    "cider_score, cider_scores = cider_obj.compute_score(\n",
    "        word_target_dict, word_response_dict)\n",
    "\n",
    "ref1_comparison = [a == b for a,b in zip(ref1_strs, sys_strs)]                  # For each question, True if extracted answer matches answer1\n",
    "ref2_comparison = [a == b for a,b in zip(ref2_strs, sys_strs)]                  # For each question, True if extracted answer matches answer2\n",
    "ref_comparison = [int(a or b) for a,b in zip(ref1_comparison, ref2_comparison)] # For each question, 1 if extracted answer matches either answer1 or answer2, else 0\n",
    "accuracy = sum(ref_comparison) / len(ref_comparison)\n",
    "\n",
    "print(\"ROUGE-L : \", round(100*rouge_score,2))\n",
    "print(\"BLEU-1  : \", round(100*bleu1_score,2))\n",
    "print(\"BLEU-4  : \", round(100*bleu4_score,2))\n",
    "print(\"METEOR  : \", round(100*meteor_score,2))\n",
    "print(\"CiDER   : \", round(100*cider_score,2))\n",
    "print(\"Accuracy: \", round(100*accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xgJD65TKHOy4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:example-python-env]",
   "language": "python",
   "name": "conda-env-example-python-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
