{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "mimir_dir = os.environ[\"MIMIR_DIR\"]\n",
    "data_dir = op.join(mimir_dir, \"data\")\n",
    "pre_dir = op.join(mimir_dir, \"preprocessed_data\")\n",
    "full_texts_dir = op.join(data_dir, \"nqa_gutenberg_corpus\")\n",
    "summary_dir = op.join(data_dir, \"nqa_summary_text_files\")\n",
    "from sentence_tokenize import file_to_sentence_tokens\n",
    "from preprocessing_pipeline import pipeline, s2v\n",
    "from ner_pipeline import *\n",
    "from utils import spacy_single_line, make_name_url_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic preprocessing example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = make_name_url_dict() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_tokens(file_path):\n",
    "    sent_tokens = file_to_sentence_tokens(file_path)\n",
    "    sent_tokens = [x for x in sent_tokens if len(x) > 0]\n",
    "    return(sent_tokens)\n",
    "\n",
    "\n",
    "def preprocess(sent_tokens, verbose = False):\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Step 1: sentence tokenization\")\n",
    "    if verbose == True:\n",
    "        print(sent_tokens[:3])\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n\\nStep 2: Word tokenization, stemming\")\n",
    "    pipelined = pipeline(sent_tokens)\n",
    "    pipelined = [x for x in pipelined if len(x) > 0]\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n\\nStep 3: Vectorize\")\n",
    "    vecs, dictionary = s2v(pipelined)\n",
    "    if verbose == True:\n",
    "        print(\"Vectors\")\n",
    "        print(list(vecs.items())[:3])\n",
    "        print(\"Idx2word dictionary\")\n",
    "        print(list(dictionary.items())[:3], \"...\")\n",
    "    \n",
    "    word2idx = {x:y for y,x in dictionary.items()}\n",
    "    \n",
    "    return(vecs, word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_list_to_ne_obj_list(sents_list, sent_tokens):\n",
    "\n",
    "    ner_function = spacy_single_line\n",
    "\n",
    "    type_threshold = 0.999  #Threshold below which we reject\n",
    "                            # a candidate Named Entity\n",
    "    token_threshold = 0.9   #Threshold below which we reject\n",
    "                            # a candidate NE in second pass\n",
    "\n",
    "    sents_list = sent_tokens\n",
    "\n",
    "    ne_token_list = first_pass(ner_function, sents_list)\n",
    "    high_confidence_types = get_high_confidence_types(ne_token_list, type_threshold)\n",
    "    obj_list = combine_types_to_entities(high_confidence_types)\n",
    "    \n",
    "    obj_dict = obj_list_to_types_dict(obj_list)\n",
    "    filtered_type_dict = get_filtered_type_dict(obj_dict)\n",
    "    obj_list = pass_2(ner_function, sents_list, filtered_type_dict, obj_list, token_threshold)\n",
    "        \n",
    "    \n",
    "    return obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing text no. 0, Bartleby, the Scrivener from train set\n",
      "Doing text no. 1, Kipps from train set\n",
      "Doing text no. 2, The Gift of the Magi from train set\n",
      "Doing text no. 3, Manalive from train set\n",
      "Doing text no. 4, The Rose and the Ring from train set\n",
      "Doing text no. 5, How He Lied to Her Husband from train set\n",
      "Doing text no. 6, A Bride of the Plains from train set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec6b4319dc00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msent_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokens_ft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msent_tokens_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mobj_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents_list_to_ne_obj_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens_ft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#There is only one \"obj list\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mvecs_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens_ft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3ec55d21e2c1>\u001b[0m in \u001b[0;36msents_list_to_ne_obj_list\u001b[0;34m(sents_list)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msents_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mne_token_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mhigh_confidence_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_high_confidence_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne_token_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mobj_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_types_to_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_confidence_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/mimir/qa/corpus_utils/ner_pipeline.py\u001b[0m in \u001b[0;36mfirst_pass\u001b[0;34m(ner_function, sents_list)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m#print(entities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/mimir/qa/corpus_utils/utils.py\u001b[0m in \u001b[0;36mspacy_single_line\u001b[0;34m(line, return_indices)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_single_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mbeams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_density\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mentity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mparses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_beam_parses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.beam_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m    163\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_dir = op.join(pre_dir, \"full_texts\")\n",
    "sm_dir = op.join(pre_dir, \"summaries\")\n",
    "\n",
    "for txt_type in [ft_dir, sm_dir]:\n",
    "    \n",
    "    if not op.exists(txt_type):\n",
    "        os.mkdir(txt_type)\n",
    "    \n",
    "    obj_list_dir = op.join(txt_type, \"ne_obj_lists\")\n",
    "    bows_dir = op.join(txt_type, \"bows\")\n",
    "    sents_dir = op.join(txt_type, \"sent_tokenized\")\n",
    "        \n",
    "    for feat_type in [obj_list_dir, bows_dir, sents_dir]:\n",
    "        if not op.exists(feat_type):\n",
    "            os.mkdir(feat_type)\n",
    "            \n",
    "        for dset in [\"train\",\"test\",\"valid\"]:\n",
    "            save_dir = op.join(feat_type, dset)\n",
    "            if not op.exists(save_dir):\n",
    "                os.mkdir(save_dir)     \n",
    "                \n",
    "                \n",
    "all_names = []                \n",
    "for dset in [\"train\",\"test\",\"valid\"]:\n",
    "    all_names += os.listdir(op.join(summary_dir, dset))\n",
    "    \n",
    "assert len(url_dict.keys()) == len(all_names) # check we have all books in our dataset\n",
    "\n",
    "           \n",
    "for dset in [\"train\",\"test\",\"valid\"]:\n",
    "    save_dir_ft_sents = op.join(ft_dir, \"sent_tokenized\", dset)\n",
    "    save_dir_ft_bows  = op.join(ft_dir, \"bows\", dset)\n",
    "    save_dir_ft_obj   = op.join(ft_dir, \"ne_obj_lists\", dset)\n",
    "    save_dir_sm_sents = op.join(sm_dir, \"sent_tokenized\", dset)\n",
    "    save_dir_sm_bows  = op.join(sm_dir, \"bows\", dset)\n",
    "    save_dir_dm_obj   = op.join(sm_dir, \"ne_obj_lists\", dset)\n",
    "    from_dir_ft       = op.join(full_texts_dir, dset)\n",
    "    from_dir_sm       = op.join(summary_dir, dset)\n",
    "    texts = os.listdir(from_dir_sm)\n",
    "    for i, text in enumerate(texts):\n",
    "        print(\"Doing text no. {}, {} from {} set\".format(i, text, dset))\n",
    "        text_path_ft = op.join(from_dir_ft, text)\n",
    "        sent_tokens_ft = get_sent_tokens(text_path_ft)\n",
    "        text_path_sm = op.join(from_dir_sm, text)\n",
    "        sent_tokens_sm = get_sent_tokens(text_path_sm)\n",
    "        sent_tokens = sent_tokens_ft + sent_tokens_sm\n",
    "        vecs, word2idx = preprocess(sent_tokens)\n",
    "        obj_list = sents_list_to_ne_obj_list(sent_tokens_ft) #There is only one \"obj list\"\n",
    "           \n",
    "        vecs_ft = {x:y for x,y in vecs.items() if x < len(sent_tokens_ft)}\n",
    "        vecs_sm = {x:y for x,y in vecs.items() if x >= len(sent_tokens_ft)}\n",
    "           \n",
    "        np.save(op.join(save_dir_ft_bows, text + \".npy\"), np.array([vecs_ft, word2idx]))\n",
    "        np.save(op.join(save_dir_sm_bows, text + \".npy\"), np.array([vecs_sm, word2idx])) \n",
    "        np.save(op.join(save_dir_ft_obj, text + \".npy\"), np.array(obj_list))\n",
    "                \n",
    "        with open(op.join(save_dir_ft_sents, text), \"w+\") as sents_file:\n",
    "            for st in sent_tokens_ft:\n",
    "                sents_file.write(st + \"\\n\")\n",
    "        with open(op.join(save_dir_sm_sents, text), \"w+\") as sents_file:\n",
    "            for st in sent_tokens_sm:\n",
    "                sents_file.write(st + \"\\n\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.join(full_texts_dir, \"train\", \"White Fang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set([obj.class_string for obj in obj_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in obj_list:\n",
    "    if obj.class_string == \"PERSON\":\n",
    "        print(obj.sents)\n",
    "        print(obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
