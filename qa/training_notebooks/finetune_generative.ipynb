{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"finetune_T5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sSC0wVWbde8R"},"source":["# Dependencies and helper functions"]},{"cell_type":"code","metadata":{"id":"oXFLDOtmbaG0","executionInfo":{"status":"ok","timestamp":1614688939252,"user_tz":0,"elapsed":24180,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["%%capture\n","!pip install transformers\n","!pip install git+https://github.com/salaniz/pycocoevalcap\n","!pip install sentencepiece"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9SWHsZFb6mP","executionInfo":{"status":"ok","timestamp":1614688939253,"user_tz":0,"elapsed":24171,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["import pandas as pd\n","import os\n","import time\n","import datetime\n","from string import punctuation\n","import pickle\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","def clean(text):\n","  '''\n","  Takes a string, removes leading and trailing whitespace,\n","  makes it lower case, and removes leading and trailing punctuation.\n","  '''\n","  text = text.strip() # remove leading and trailing whitespace\n","  text = text.lower() # lower case\n","  text = text.strip(punctuation)\n","\n","  return text"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RF3o2uw1dpwt"},"source":["# Data Pre-processing\n","In this section, we load the data required for training the model and perform any appropriate filtering/pre-processing"]},{"cell_type":"code","metadata":{"id":"dJBqfwr4bAHZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614688965889,"user_tz":0,"elapsed":50800,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}},"outputId":"d77b9ecc-a3c4-4d97-b035-b9ac11a21b29"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","ROOT        = 'gdrive/Shared drives/CDT Mini-Project Team 1/Colab Notebooks/'\n","DATA_DIR    = ROOT + 'data/' \n","MODELS_DIR  = ROOT + 'models/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MF2jgHfAbO21","executionInfo":{"status":"ok","timestamp":1614688968433,"user_tz":0,"elapsed":53337,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["#Load all data\n","qaps = pd.read_csv(DATA_DIR + 'narrativeqa_qas.csv')\n","summaries = pd.read_csv(DATA_DIR + 'summaries.csv')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5sp8-EAErHR","executionInfo":{"status":"ok","timestamp":1614688968434,"user_tz":0,"elapsed":53333,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["summaries = summaries.set_index('document_id')\n","summaries = summaries.drop(labels=['set','summary_tokenized'],axis='columns')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLu_De1S0y4q","executionInfo":{"status":"ok","timestamp":1614688968435,"user_tz":0,"elapsed":53328,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["qaps = qaps.set_index('document_id')\n","qaps = qaps.drop(labels=['question_tokenized','answer1_tokenized','answer2_tokenized'], axis='columns')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DemWvy7GKi3","executionInfo":{"status":"ok","timestamp":1614688968435,"user_tz":0,"elapsed":53323,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["# pair qaps with their relevant summaries and drop non-tokenized fields\n","qaps = qaps.join(summaries)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"-zlM1Fy3RADS","executionInfo":{"status":"ok","timestamp":1614688968436,"user_tz":0,"elapsed":53318,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}},"outputId":"095f39da-42ed-424a-ec84-8657b11be34c"},"source":["qaps.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>set</th>\n","      <th>question</th>\n","      <th>answer1</th>\n","      <th>answer2</th>\n","      <th>summary</th>\n","    </tr>\n","    <tr>\n","      <th>document_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0025577043f5090cd603c6aea60f26e236195594</th>\n","      <td>test</td>\n","      <td>Who is Mark Hunter?</td>\n","      <td>He is a high school student in Phoenix.</td>\n","      <td>A loner and outsider student with a radio stat...</td>\n","      <td>Mark Hunter (Slater), a high school student i...</td>\n","    </tr>\n","    <tr>\n","      <th>0025577043f5090cd603c6aea60f26e236195594</th>\n","      <td>test</td>\n","      <td>Where does this radio station take place?</td>\n","      <td>It takes place in Mark's parents basement.</td>\n","      <td>Phoenix, Arizona</td>\n","      <td>Mark Hunter (Slater), a high school student i...</td>\n","    </tr>\n","    <tr>\n","      <th>0025577043f5090cd603c6aea60f26e236195594</th>\n","      <td>test</td>\n","      <td>Why do more students tune into Mark's show?</td>\n","      <td>Mark talks about what goes on at school and in...</td>\n","      <td>Because he has a thing to say about what is ha...</td>\n","      <td>Mark Hunter (Slater), a high school student i...</td>\n","    </tr>\n","    <tr>\n","      <th>0025577043f5090cd603c6aea60f26e236195594</th>\n","      <td>test</td>\n","      <td>Who commits suicide?</td>\n","      <td>Malcolm.</td>\n","      <td>Malcolm.</td>\n","      <td>Mark Hunter (Slater), a high school student i...</td>\n","    </tr>\n","    <tr>\n","      <th>0025577043f5090cd603c6aea60f26e236195594</th>\n","      <td>test</td>\n","      <td>What does Paige jam into her microwave?</td>\n","      <td>She jams her medals and accolades.</td>\n","      <td>Her award medals</td>\n","      <td>Mark Hunter (Slater), a high school student i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           set  ...                                            summary\n","document_id                                     ...                                                   \n","0025577043f5090cd603c6aea60f26e236195594  test  ...   Mark Hunter (Slater), a high school student i...\n","0025577043f5090cd603c6aea60f26e236195594  test  ...   Mark Hunter (Slater), a high school student i...\n","0025577043f5090cd603c6aea60f26e236195594  test  ...   Mark Hunter (Slater), a high school student i...\n","0025577043f5090cd603c6aea60f26e236195594  test  ...   Mark Hunter (Slater), a high school student i...\n","0025577043f5090cd603c6aea60f26e236195594  test  ...   Mark Hunter (Slater), a high school student i...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"fKc2Rc8Vg7Uf"},"source":["Acquire data in lists of: contexts (summaries), questions, answers"]},{"cell_type":"code","metadata":{"id":"bTHQ7ufUbYlB","executionInfo":{"status":"ok","timestamp":1614688971485,"user_tz":0,"elapsed":56360,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["def format_data(data):\n","  contexts = []\n","  questions = []\n","  answers = []\n","  for index, row in data.iterrows():\n","    context   = 'context: ' + row['summary']\n","    question  = 'question: ' + row['question']\n","    answer    = {}\n","    answer['answer1'] = clean(row['answer1'])\n","    answer['answer2'] = clean(row['answer2'])\n","\n","    contexts.append(context)\n","    questions.append(question)\n","    answers.append(answer)\n","  \n","  return contexts, questions, answers\n","\n","train_contexts, train_questions, train_answers = format_data(qaps[qaps['set']=='train'])\n","val_contexts, val_questions, val_answers       = format_data(qaps[qaps['set']=='valid'])\n","\n","dummy_questions = 100* ['How big is the Empire State Building?', 'Who is Shrek married to?']\n","dummy_contexts = 100 * ['The Empire State building is a very big building. It is one of the biggest buildings in the world. It is large.', 'Shrek is an ogre. There is a common misconception that Shrek is married to Donkey, but he is actually married to Fiona, the princess.']\n","dummy_answers = 100 * [{'answer1': 'very big', 'answer2': 'large'}, {'answer1': 'Fiona', 'answer2': 'the princess'}]"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RWtqM9_6H5X"},"source":["## Tokenization and Data Preparation"]},{"cell_type":"code","metadata":{"id":"1zKWSniaJ0OU","executionInfo":{"status":"ok","timestamp":1614689002223,"user_tz":0,"elapsed":87093,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["from transformers import T5TokenizerFast, T5ForConditionalGeneration\n","import torch\n","\n","MODEL_ID = 'T5_base_finetuned/epoch1'\n","\n","tokenizer = T5TokenizerFast.from_pretrained(MODELS_DIR+MODEL_ID)\n","model = T5ForConditionalGeneration.from_pretrained(MODELS_DIR+MODEL_ID, output_attentions=False, output_hidden_states=False)\n","\n","MODEL_ID = 'T5_base_finetuned'\n","\n","batch_size = 2\n","training_stats = []\n","current_epoch = 1\n","num_of_epochs = 3\n","lr = 1e-5"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srXAIIHP6gdQ","executionInfo":{"status":"ok","timestamp":1614689096241,"user_tz":0,"elapsed":181105,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}},"outputId":"b8fb426b-0059-4211-980e-1be872654170"},"source":["def tokenize_data(questions, contexts, answers, tokenizer):\n","\n","  input_ids       = []\n","  labels          = []\n","\n","  for q, c, a in zip(questions, contexts, answers):\n","    question_input_ids = tokenizer.encode(q + '\\t' + c, max_length=1024, padding='max_length', truncation=True, return_tensors='pt')\n","    label_input_ids_1  = tokenizer.encode(a['answer1'], return_tensors='pt', max_length=100, truncation=True, padding='max_length')\n","    label_input_ids_2  = tokenizer.encode(a['answer2'], return_tensors='pt', max_length=100, truncation=True, padding='max_length')\n","\n","    input_ids.append(torch.tensor(question_input_ids))\n","    input_ids.append(torch.tensor(question_input_ids))\n","    labels.append(torch.tensor(label_input_ids_1))\n","    labels.append(torch.tensor(label_input_ids_2))\n","\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  labels    = torch.cat(labels, dim=0)\n","\n","  return torch.utils.data.TensorDataset(input_ids, labels)\n","\n","dummy_dataset = tokenize_data(dummy_questions, dummy_contexts, dummy_answers, tokenizer)\n","train_dataset = tokenize_data(train_questions, train_contexts, train_answers, tokenizer)\n","val_dataset = tokenize_data(val_questions, val_contexts, val_answers, tokenizer)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"GpW_n8Ll6b_7"},"source":["## Model Training"]},{"cell_type":"code","metadata":{"id":"qrpc0djxBQmS","executionInfo":{"status":"ok","timestamp":1614689096243,"user_tz":0,"elapsed":181101,"user":{"displayName":"Kyle G Reed","photoUrl":"","userId":"01028854829176702804"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW\n","\n","# create DataLoaders\n","train_loader = DataLoader(train_dataset,\n","                          sampler = RandomSampler(train_dataset),\n","                          batch_size=batch_size)\n","\n","# For dev the order doesn't matter, so we'll just read them sequentially.\n","val_loader = DataLoader(\n","            val_dataset, # The dev samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","# initialise optimizer\n","optim = AdamW(model.parameters(), \n","              lr=lr,\n","              eps = 1e-8)\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","total_steps = len(train_loader) * (num_of_epochs - current_epoch)\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optim, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtj0R5re6Mwl","outputId":"ef434ff0-87fe-4bef-ec89-76a2b005726e"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","if torch.cuda.is_available() : model.cuda()\n","model.to(device)\n","\n","# Measure how long the training epoch takes.\n","total_t0 = time.time()\n","\n","for epoch in range(current_epoch, num_of_epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_of_epochs))\n","    print('Training...')  \n","\n","    # Measure how long training epoch takes\n","    t0 = time.time()\n","\n","    # Reset loss for epoch\n","    total_train_loss = 0\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_loader):\n","        #print(batch)\n","\n","        # Progress update every 100 batches.\n","        if (step % 100 == 0 or step < 6) and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n","\n","        optim.zero_grad()\n","        input_ids = batch[0].to(device)\n","        labels    = batch[1].to(device)\n","\n","        outputs = model(input_ids=input_ids, labels=labels)\n","        loss = outputs.loss\n","\n","        # accumulate training loss over batches\n","        total_train_loss += loss.item()\n","\n","        # clip norm of gradients to 1.0 to address vanishing gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        loss.backward()\n","        optim.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_loader)\n","    training_time = format_time(time.time() - t0)\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","\n","\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running validation...\")\n","\n","    total_val_loss = 0\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Evaluate data for one epoch\n","    for batch in val_loader:\n","\n","        input_ids = batch[0].to(device)\n","        labels    = batch[1].to(device)\n","\n","        # Don't construct a compute graph (only required for backprop during training)\n","        with torch.no_grad():\n","          outputs = model(input_ids=input_ids, labels=labels)\n","\n","        loss = outputs.loss\n","        total_val_loss += loss.item()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_val_loss / len(val_loader)\n","\n","    # Measure how long the dev run took.\n","    dev_time = format_time(time.time() - t0)\n","    \n","    print(\"  Dev Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Dev took: {:}\".format(dev_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch + 1,\n","            'training_loss': avg_train_loss,\n","            'validation_loss': avg_val_loss,\n","            'total_loss': avg_train_loss + avg_val_loss\n","        }\n","    )\n","\n","    print(\"\")\n","    print(\"Saving model\")\n","\n","    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","    # They can then be reloaded using `from_pretrained()`\n","    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","\n","    MODEL_PATH = MODELS_DIR + MODEL_ID + '/epoch' + str(epoch + 1)\n","\n","    if not os.path.exists(MODEL_PATH):\n","        os.makedirs(MODEL_PATH)\n","\n","    model_to_save.save_pretrained(MODEL_PATH)\n","    tokenizer.save_pretrained(MODEL_PATH)\n","    with open(MODEL_PATH + '/stats', \"wb\") as stats:\n","      pickle.dump(training_stats, stats)\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch     1  of  32,747.    Elapsed: 0:00:01.\n","  Batch     2  of  32,747.    Elapsed: 0:00:02.\n","  Batch     3  of  32,747.    Elapsed: 0:00:02.\n","  Batch     4  of  32,747.    Elapsed: 0:00:03.\n","  Batch     5  of  32,747.    Elapsed: 0:00:04.\n","  Batch   100  of  32,747.    Elapsed: 0:01:14.\n","  Batch   200  of  32,747.    Elapsed: 0:02:30.\n","  Batch   300  of  32,747.    Elapsed: 0:03:47.\n","  Batch   400  of  32,747.    Elapsed: 0:05:06.\n","  Batch   500  of  32,747.    Elapsed: 0:06:25.\n","  Batch   600  of  32,747.    Elapsed: 0:07:45.\n","  Batch   700  of  32,747.    Elapsed: 0:09:04.\n","  Batch   800  of  32,747.    Elapsed: 0:10:23.\n","  Batch   900  of  32,747.    Elapsed: 0:11:43.\n","  Batch 1,000  of  32,747.    Elapsed: 0:13:02.\n","  Batch 1,100  of  32,747.    Elapsed: 0:14:21.\n","  Batch 1,200  of  32,747.    Elapsed: 0:15:40.\n","  Batch 1,300  of  32,747.    Elapsed: 0:17:00.\n","  Batch 1,400  of  32,747.    Elapsed: 0:18:19.\n","  Batch 1,500  of  32,747.    Elapsed: 0:19:38.\n","  Batch 1,600  of  32,747.    Elapsed: 0:20:58.\n","  Batch 1,700  of  32,747.    Elapsed: 0:22:17.\n","  Batch 1,800  of  32,747.    Elapsed: 0:23:36.\n","  Batch 1,900  of  32,747.    Elapsed: 0:24:56.\n","  Batch 2,000  of  32,747.    Elapsed: 0:26:15.\n","  Batch 2,100  of  32,747.    Elapsed: 0:27:34.\n","  Batch 2,200  of  32,747.    Elapsed: 0:28:53.\n","  Batch 2,300  of  32,747.    Elapsed: 0:30:13.\n","  Batch 2,400  of  32,747.    Elapsed: 0:31:32.\n","  Batch 2,500  of  32,747.    Elapsed: 0:32:51.\n","  Batch 2,600  of  32,747.    Elapsed: 0:34:10.\n","  Batch 2,700  of  32,747.    Elapsed: 0:35:30.\n","  Batch 2,800  of  32,747.    Elapsed: 0:36:49.\n","  Batch 2,900  of  32,747.    Elapsed: 0:38:08.\n","  Batch 3,000  of  32,747.    Elapsed: 0:39:28.\n","  Batch 3,100  of  32,747.    Elapsed: 0:40:47.\n","  Batch 3,200  of  32,747.    Elapsed: 0:42:06.\n","  Batch 3,300  of  32,747.    Elapsed: 0:43:26.\n","  Batch 3,400  of  32,747.    Elapsed: 0:44:45.\n","  Batch 3,500  of  32,747.    Elapsed: 0:46:04.\n","  Batch 3,600  of  32,747.    Elapsed: 0:47:24.\n","  Batch 3,700  of  32,747.    Elapsed: 0:48:43.\n","  Batch 3,800  of  32,747.    Elapsed: 0:50:02.\n","  Batch 3,900  of  32,747.    Elapsed: 0:51:22.\n","  Batch 4,000  of  32,747.    Elapsed: 0:52:41.\n","  Batch 4,100  of  32,747.    Elapsed: 0:54:00.\n","  Batch 4,200  of  32,747.    Elapsed: 0:55:19.\n","  Batch 4,300  of  32,747.    Elapsed: 0:56:38.\n","  Batch 4,400  of  32,747.    Elapsed: 0:57:58.\n","  Batch 4,500  of  32,747.    Elapsed: 0:59:17.\n","  Batch 4,600  of  32,747.    Elapsed: 1:00:36.\n","  Batch 4,700  of  32,747.    Elapsed: 1:01:56.\n","  Batch 4,800  of  32,747.    Elapsed: 1:03:15.\n","  Batch 4,900  of  32,747.    Elapsed: 1:04:34.\n","  Batch 5,000  of  32,747.    Elapsed: 1:05:53.\n","  Batch 5,100  of  32,747.    Elapsed: 1:07:13.\n","  Batch 5,200  of  32,747.    Elapsed: 1:08:32.\n","  Batch 5,300  of  32,747.    Elapsed: 1:09:51.\n","  Batch 5,400  of  32,747.    Elapsed: 1:11:11.\n","  Batch 5,500  of  32,747.    Elapsed: 1:12:30.\n","  Batch 5,600  of  32,747.    Elapsed: 1:13:49.\n","  Batch 5,700  of  32,747.    Elapsed: 1:15:08.\n","  Batch 5,800  of  32,747.    Elapsed: 1:16:28.\n","  Batch 5,900  of  32,747.    Elapsed: 1:17:47.\n","  Batch 6,000  of  32,747.    Elapsed: 1:19:06.\n","  Batch 6,100  of  32,747.    Elapsed: 1:20:25.\n","  Batch 6,200  of  32,747.    Elapsed: 1:21:45.\n","  Batch 6,300  of  32,747.    Elapsed: 1:23:04.\n","  Batch 6,400  of  32,747.    Elapsed: 1:24:23.\n","  Batch 6,500  of  32,747.    Elapsed: 1:25:42.\n","  Batch 6,600  of  32,747.    Elapsed: 1:27:01.\n","  Batch 6,700  of  32,747.    Elapsed: 1:28:20.\n","  Batch 6,800  of  32,747.    Elapsed: 1:29:39.\n","  Batch 6,900  of  32,747.    Elapsed: 1:30:58.\n","  Batch 7,000  of  32,747.    Elapsed: 1:32:17.\n","  Batch 7,100  of  32,747.    Elapsed: 1:33:36.\n","  Batch 7,200  of  32,747.    Elapsed: 1:34:55.\n","  Batch 7,300  of  32,747.    Elapsed: 1:36:14.\n","  Batch 7,400  of  32,747.    Elapsed: 1:37:33.\n","  Batch 7,500  of  32,747.    Elapsed: 1:38:52.\n","  Batch 7,600  of  32,747.    Elapsed: 1:40:11.\n","  Batch 7,700  of  32,747.    Elapsed: 1:41:30.\n","  Batch 7,800  of  32,747.    Elapsed: 1:42:49.\n","  Batch 7,900  of  32,747.    Elapsed: 1:44:08.\n","  Batch 8,000  of  32,747.    Elapsed: 1:45:27.\n","  Batch 8,100  of  32,747.    Elapsed: 1:46:46.\n","  Batch 8,200  of  32,747.    Elapsed: 1:48:04.\n","  Batch 8,300  of  32,747.    Elapsed: 1:49:23.\n","  Batch 8,400  of  32,747.    Elapsed: 1:50:42.\n","  Batch 8,500  of  32,747.    Elapsed: 1:52:01.\n","  Batch 8,600  of  32,747.    Elapsed: 1:53:20.\n","  Batch 8,700  of  32,747.    Elapsed: 1:54:39.\n","  Batch 8,800  of  32,747.    Elapsed: 1:55:58.\n","  Batch 8,900  of  32,747.    Elapsed: 1:57:17.\n","  Batch 9,000  of  32,747.    Elapsed: 1:58:36.\n","  Batch 9,100  of  32,747.    Elapsed: 1:59:55.\n","  Batch 9,200  of  32,747.    Elapsed: 2:01:14.\n","  Batch 9,300  of  32,747.    Elapsed: 2:02:33.\n","  Batch 9,400  of  32,747.    Elapsed: 2:03:52.\n","  Batch 9,500  of  32,747.    Elapsed: 2:05:10.\n","  Batch 9,600  of  32,747.    Elapsed: 2:06:29.\n","  Batch 9,700  of  32,747.    Elapsed: 2:07:48.\n","  Batch 9,800  of  32,747.    Elapsed: 2:09:07.\n","  Batch 9,900  of  32,747.    Elapsed: 2:10:26.\n","  Batch 10,000  of  32,747.    Elapsed: 2:11:45.\n","  Batch 10,100  of  32,747.    Elapsed: 2:13:04.\n","  Batch 10,200  of  32,747.    Elapsed: 2:14:22.\n","  Batch 10,300  of  32,747.    Elapsed: 2:15:41.\n","  Batch 10,400  of  32,747.    Elapsed: 2:17:00.\n","  Batch 10,500  of  32,747.    Elapsed: 2:18:19.\n","  Batch 10,600  of  32,747.    Elapsed: 2:19:38.\n","  Batch 10,700  of  32,747.    Elapsed: 2:20:57.\n","  Batch 10,800  of  32,747.    Elapsed: 2:22:16.\n","  Batch 10,900  of  32,747.    Elapsed: 2:23:35.\n","  Batch 11,000  of  32,747.    Elapsed: 2:24:54.\n","  Batch 11,100  of  32,747.    Elapsed: 2:26:13.\n","  Batch 11,200  of  32,747.    Elapsed: 2:27:31.\n","  Batch 11,300  of  32,747.    Elapsed: 2:28:50.\n","  Batch 11,400  of  32,747.    Elapsed: 2:30:09.\n","  Batch 11,500  of  32,747.    Elapsed: 2:31:28.\n","  Batch 11,600  of  32,747.    Elapsed: 2:32:47.\n","  Batch 11,700  of  32,747.    Elapsed: 2:34:06.\n","  Batch 11,800  of  32,747.    Elapsed: 2:35:25.\n","  Batch 11,900  of  32,747.    Elapsed: 2:36:44.\n","  Batch 12,000  of  32,747.    Elapsed: 2:38:03.\n","  Batch 12,100  of  32,747.    Elapsed: 2:39:22.\n","  Batch 12,200  of  32,747.    Elapsed: 2:40:41.\n","  Batch 12,300  of  32,747.    Elapsed: 2:42:00.\n","  Batch 12,400  of  32,747.    Elapsed: 2:43:19.\n","  Batch 12,500  of  32,747.    Elapsed: 2:44:38.\n","  Batch 12,600  of  32,747.    Elapsed: 2:45:56.\n","  Batch 12,700  of  32,747.    Elapsed: 2:47:15.\n","  Batch 12,800  of  32,747.    Elapsed: 2:48:34.\n","  Batch 12,900  of  32,747.    Elapsed: 2:49:53.\n","  Batch 13,000  of  32,747.    Elapsed: 2:51:12.\n","  Batch 13,100  of  32,747.    Elapsed: 2:52:31.\n","  Batch 13,200  of  32,747.    Elapsed: 2:53:50.\n","  Batch 13,300  of  32,747.    Elapsed: 2:55:08.\n","  Batch 13,400  of  32,747.    Elapsed: 2:56:27.\n","  Batch 13,500  of  32,747.    Elapsed: 2:57:46.\n","  Batch 13,600  of  32,747.    Elapsed: 2:59:05.\n","  Batch 13,700  of  32,747.    Elapsed: 3:00:24.\n","  Batch 13,800  of  32,747.    Elapsed: 3:01:43.\n","  Batch 13,900  of  32,747.    Elapsed: 3:03:02.\n","  Batch 14,000  of  32,747.    Elapsed: 3:04:20.\n","  Batch 14,100  of  32,747.    Elapsed: 3:05:39.\n","  Batch 14,200  of  32,747.    Elapsed: 3:06:58.\n","  Batch 14,300  of  32,747.    Elapsed: 3:08:17.\n","  Batch 14,400  of  32,747.    Elapsed: 3:09:36.\n","  Batch 14,500  of  32,747.    Elapsed: 3:10:55.\n","  Batch 14,600  of  32,747.    Elapsed: 3:12:13.\n","  Batch 14,700  of  32,747.    Elapsed: 3:13:32.\n","  Batch 14,800  of  32,747.    Elapsed: 3:14:51.\n","  Batch 14,900  of  32,747.    Elapsed: 3:16:10.\n","  Batch 15,000  of  32,747.    Elapsed: 3:17:29.\n","  Batch 15,100  of  32,747.    Elapsed: 3:18:48.\n","  Batch 15,200  of  32,747.    Elapsed: 3:20:07.\n","  Batch 15,300  of  32,747.    Elapsed: 3:21:26.\n","  Batch 15,400  of  32,747.    Elapsed: 3:22:45.\n","  Batch 15,500  of  32,747.    Elapsed: 3:24:04.\n","  Batch 15,600  of  32,747.    Elapsed: 3:25:23.\n","  Batch 15,700  of  32,747.    Elapsed: 3:26:41.\n","  Batch 15,800  of  32,747.    Elapsed: 3:28:00.\n","  Batch 15,900  of  32,747.    Elapsed: 3:29:19.\n","  Batch 16,000  of  32,747.    Elapsed: 3:30:39.\n","  Batch 16,100  of  32,747.    Elapsed: 3:31:57.\n","  Batch 16,200  of  32,747.    Elapsed: 3:33:16.\n","  Batch 16,300  of  32,747.    Elapsed: 3:34:35.\n","  Batch 16,400  of  32,747.    Elapsed: 3:35:54.\n","  Batch 16,500  of  32,747.    Elapsed: 3:37:13.\n","  Batch 16,600  of  32,747.    Elapsed: 3:38:32.\n","  Batch 16,700  of  32,747.    Elapsed: 3:39:51.\n","  Batch 16,800  of  32,747.    Elapsed: 3:41:09.\n","  Batch 16,900  of  32,747.    Elapsed: 3:42:28.\n","  Batch 17,000  of  32,747.    Elapsed: 3:43:47.\n","  Batch 17,100  of  32,747.    Elapsed: 3:45:06.\n","  Batch 17,200  of  32,747.    Elapsed: 3:46:25.\n","  Batch 17,300  of  32,747.    Elapsed: 3:47:44.\n","  Batch 17,400  of  32,747.    Elapsed: 3:49:03.\n","  Batch 17,500  of  32,747.    Elapsed: 3:50:22.\n","  Batch 17,600  of  32,747.    Elapsed: 3:51:41.\n","  Batch 17,700  of  32,747.    Elapsed: 3:52:59.\n","  Batch 17,800  of  32,747.    Elapsed: 3:54:18.\n","  Batch 17,900  of  32,747.    Elapsed: 3:55:37.\n","  Batch 18,000  of  32,747.    Elapsed: 3:56:56.\n","  Batch 18,100  of  32,747.    Elapsed: 3:58:15.\n","  Batch 18,200  of  32,747.    Elapsed: 3:59:34.\n","  Batch 18,300  of  32,747.    Elapsed: 4:00:53.\n","  Batch 18,400  of  32,747.    Elapsed: 4:02:13.\n","  Batch 18,500  of  32,747.    Elapsed: 4:03:32.\n","  Batch 18,600  of  32,747.    Elapsed: 4:04:50.\n","  Batch 18,700  of  32,747.    Elapsed: 4:06:09.\n","  Batch 18,800  of  32,747.    Elapsed: 4:07:28.\n","  Batch 18,900  of  32,747.    Elapsed: 4:08:47.\n","  Batch 19,000  of  32,747.    Elapsed: 4:10:06.\n","  Batch 19,100  of  32,747.    Elapsed: 4:11:25.\n","  Batch 19,200  of  32,747.    Elapsed: 4:12:43.\n","  Batch 19,300  of  32,747.    Elapsed: 4:14:02.\n","  Batch 19,400  of  32,747.    Elapsed: 4:15:21.\n","  Batch 19,500  of  32,747.    Elapsed: 4:16:40.\n","  Batch 19,600  of  32,747.    Elapsed: 4:17:58.\n","  Batch 19,700  of  32,747.    Elapsed: 4:19:17.\n","  Batch 19,800  of  32,747.    Elapsed: 4:20:36.\n","  Batch 19,900  of  32,747.    Elapsed: 4:21:55.\n","  Batch 20,000  of  32,747.    Elapsed: 4:23:14.\n","  Batch 20,100  of  32,747.    Elapsed: 4:24:32.\n","  Batch 20,200  of  32,747.    Elapsed: 4:25:51.\n","  Batch 20,300  of  32,747.    Elapsed: 4:27:10.\n","  Batch 20,400  of  32,747.    Elapsed: 4:28:28.\n","  Batch 20,500  of  32,747.    Elapsed: 4:29:47.\n","  Batch 20,600  of  32,747.    Elapsed: 4:31:06.\n","  Batch 20,700  of  32,747.    Elapsed: 4:32:25.\n","  Batch 20,800  of  32,747.    Elapsed: 4:33:44.\n","  Batch 20,900  of  32,747.    Elapsed: 4:35:03.\n","  Batch 21,000  of  32,747.    Elapsed: 4:36:21.\n","  Batch 21,100  of  32,747.    Elapsed: 4:37:40.\n","  Batch 21,200  of  32,747.    Elapsed: 4:38:59.\n","  Batch 21,300  of  32,747.    Elapsed: 4:40:18.\n","  Batch 21,400  of  32,747.    Elapsed: 4:41:37.\n","  Batch 21,500  of  32,747.    Elapsed: 4:42:56.\n","  Batch 21,600  of  32,747.    Elapsed: 4:44:15.\n","  Batch 21,700  of  32,747.    Elapsed: 4:45:34.\n","  Batch 21,800  of  32,747.    Elapsed: 4:46:52.\n","  Batch 21,900  of  32,747.    Elapsed: 4:48:11.\n","  Batch 22,000  of  32,747.    Elapsed: 4:49:30.\n","  Batch 22,100  of  32,747.    Elapsed: 4:50:49.\n","  Batch 22,200  of  32,747.    Elapsed: 4:52:07.\n","  Batch 22,300  of  32,747.    Elapsed: 4:53:26.\n","  Batch 22,400  of  32,747.    Elapsed: 4:54:45.\n","  Batch 22,500  of  32,747.    Elapsed: 4:56:04.\n","  Batch 22,600  of  32,747.    Elapsed: 4:57:22.\n","  Batch 22,700  of  32,747.    Elapsed: 4:58:41.\n","  Batch 22,800  of  32,747.    Elapsed: 5:00:00.\n","  Batch 22,900  of  32,747.    Elapsed: 5:01:19.\n","  Batch 23,000  of  32,747.    Elapsed: 5:02:37.\n","  Batch 23,100  of  32,747.    Elapsed: 5:03:56.\n","  Batch 23,200  of  32,747.    Elapsed: 5:05:15.\n","  Batch 23,300  of  32,747.    Elapsed: 5:06:34.\n","  Batch 23,400  of  32,747.    Elapsed: 5:07:52.\n","  Batch 23,500  of  32,747.    Elapsed: 5:09:11.\n","  Batch 23,600  of  32,747.    Elapsed: 5:10:30.\n","  Batch 23,700  of  32,747.    Elapsed: 5:11:48.\n","  Batch 23,800  of  32,747.    Elapsed: 5:13:07.\n","  Batch 23,900  of  32,747.    Elapsed: 5:14:26.\n","  Batch 24,000  of  32,747.    Elapsed: 5:15:45.\n","  Batch 24,100  of  32,747.    Elapsed: 5:17:04.\n","  Batch 24,200  of  32,747.    Elapsed: 5:18:22.\n","  Batch 24,300  of  32,747.    Elapsed: 5:19:41.\n","  Batch 24,400  of  32,747.    Elapsed: 5:21:00.\n","  Batch 24,500  of  32,747.    Elapsed: 5:22:19.\n","  Batch 24,600  of  32,747.    Elapsed: 5:23:38.\n","  Batch 24,700  of  32,747.    Elapsed: 5:24:57.\n","  Batch 24,800  of  32,747.    Elapsed: 5:26:16.\n","  Batch 24,900  of  32,747.    Elapsed: 5:27:34.\n","  Batch 25,000  of  32,747.    Elapsed: 5:28:53.\n","  Batch 25,100  of  32,747.    Elapsed: 5:30:12.\n","  Batch 25,200  of  32,747.    Elapsed: 5:31:31.\n","  Batch 25,300  of  32,747.    Elapsed: 5:32:50.\n","  Batch 25,400  of  32,747.    Elapsed: 5:34:09.\n","  Batch 25,500  of  32,747.    Elapsed: 5:35:27.\n","  Batch 25,600  of  32,747.    Elapsed: 5:36:46.\n","  Batch 25,700  of  32,747.    Elapsed: 5:38:05.\n","  Batch 25,800  of  32,747.    Elapsed: 5:39:24.\n","  Batch 25,900  of  32,747.    Elapsed: 5:40:43.\n","  Batch 26,000  of  32,747.    Elapsed: 5:42:02.\n","  Batch 26,100  of  32,747.    Elapsed: 5:43:21.\n","  Batch 26,200  of  32,747.    Elapsed: 5:44:40.\n","  Batch 26,300  of  32,747.    Elapsed: 5:45:59.\n","  Batch 26,400  of  32,747.    Elapsed: 5:47:18.\n","  Batch 26,500  of  32,747.    Elapsed: 5:48:36.\n","  Batch 26,600  of  32,747.    Elapsed: 5:49:55.\n","  Batch 26,700  of  32,747.    Elapsed: 5:51:14.\n","  Batch 26,800  of  32,747.    Elapsed: 5:52:33.\n","  Batch 26,900  of  32,747.    Elapsed: 5:53:51.\n","  Batch 27,000  of  32,747.    Elapsed: 5:55:10.\n","  Batch 27,100  of  32,747.    Elapsed: 5:56:29.\n","  Batch 27,200  of  32,747.    Elapsed: 5:57:48.\n","  Batch 27,300  of  32,747.    Elapsed: 5:59:06.\n","  Batch 27,400  of  32,747.    Elapsed: 6:00:25.\n","  Batch 27,500  of  32,747.    Elapsed: 6:01:44.\n","  Batch 27,600  of  32,747.    Elapsed: 6:03:03.\n","  Batch 27,700  of  32,747.    Elapsed: 6:04:22.\n","  Batch 27,800  of  32,747.    Elapsed: 6:05:40.\n","  Batch 27,900  of  32,747.    Elapsed: 6:06:59.\n","  Batch 28,000  of  32,747.    Elapsed: 6:08:18.\n","  Batch 28,100  of  32,747.    Elapsed: 6:09:36.\n","  Batch 28,200  of  32,747.    Elapsed: 6:10:55.\n","  Batch 28,300  of  32,747.    Elapsed: 6:12:14.\n","  Batch 28,400  of  32,747.    Elapsed: 6:13:33.\n","  Batch 28,500  of  32,747.    Elapsed: 6:14:52.\n","  Batch 28,600  of  32,747.    Elapsed: 6:16:11.\n","  Batch 28,700  of  32,747.    Elapsed: 6:17:29.\n","  Batch 28,800  of  32,747.    Elapsed: 6:18:48.\n","  Batch 28,900  of  32,747.    Elapsed: 6:20:07.\n","  Batch 29,000  of  32,747.    Elapsed: 6:21:26.\n","  Batch 29,100  of  32,747.    Elapsed: 6:22:44.\n","  Batch 29,200  of  32,747.    Elapsed: 6:24:03.\n","  Batch 29,300  of  32,747.    Elapsed: 6:25:22.\n","  Batch 29,400  of  32,747.    Elapsed: 6:26:41.\n","  Batch 29,500  of  32,747.    Elapsed: 6:28:00.\n","  Batch 29,600  of  32,747.    Elapsed: 6:29:18.\n","  Batch 29,700  of  32,747.    Elapsed: 6:30:37.\n","  Batch 29,800  of  32,747.    Elapsed: 6:31:56.\n","  Batch 29,900  of  32,747.    Elapsed: 6:33:14.\n","  Batch 30,000  of  32,747.    Elapsed: 6:34:33.\n","  Batch 30,100  of  32,747.    Elapsed: 6:35:52.\n","  Batch 30,200  of  32,747.    Elapsed: 6:37:11.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uMUa9t0jC0Zd"},"source":["## Model stats"]},{"cell_type":"code","metadata":{"id":"w6aqdkE3C2Dg"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhO3WIC0C5UF"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['training_loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['validation_loss'], 'g-o', label=\"Validation\")\n","plt.plot(df_stats['total_loss'], 'y-o', label=\"Total\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.xticks(range(1, num_of_epochs+1))\n","plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), shadow=True, ncol=2)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtQmQ4DvC_yX"},"source":["## Sandbox"]},{"cell_type":"code","metadata":{"id":"ZULkkAGcR97v"},"source":["for question, answer in zip(dummy_questions, generated_answers):\n","  print('Question : {}'.format(question))\n","  print('Answer   : {}\\n'.format(answer))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuBLd7F5A0mx"},"source":["# Evaluation metrics"]},{"cell_type":"code","metadata":{"id":"1K9Qvxc4BkbA"},"source":["from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.cider.cider import Cider\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.bleu.bleu import Bleu\n","\n","meteor_obj = Meteor()\n","rouge_obj = Rouge()\n","cider_obj = Cider()\n","bleu_obj = Bleu(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N59hqiELDdbN"},"source":["ref1_strs = [answer['answer1'] for answer in answers]\n","ref2_strs = [answer['answer2'] for answer in answers]\n","sys_strs  = [answer['generated_answer'] for answer in answers]\n","\n","assert len(ref1_strs) == len(ref2_strs)\n","assert len(ref2_strs) == len(sys_strs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dU8A1T3PFFR4"},"source":["word_target_dict = {}\n","word_response_dict = {}\n","\n","for i in range(len(ref1_strs)):\n","    word_target_dict[i] = [ref1_strs[i], ref2_strs[i]]\n","    word_response_dict[i] = [sys_strs[i]]\n","\n","\n","bleu_score, bleu_scores = bleu_obj.compute_score(\n","        word_target_dict, word_response_dict,\n","        verbose=False)\n","bleu1_score, _, _, bleu4_score = bleu_score\n","bleu1_scores, _, _, bleu4_scores = bleu_scores\n","meteor_score, meteor_scores = meteor_obj.compute_score(\n","        word_target_dict, word_response_dict) \n","rouge_score, rouge_scores = rouge_obj.compute_score(\n","        word_target_dict, word_response_dict) \n","cider_score, cider_scores = cider_obj.compute_score(\n","        word_target_dict, word_response_dict)\n","\n","ref1_comparison = [a == b for a,b in zip(ref1_strs, sys_strs)]                  # For each question, True if extracted answer matches answer1\n","ref2_comparison = [a == b for a,b in zip(ref2_strs, sys_strs)]                  # For each question, True if extracted answer matches answer2\n","ref_comparison = [int(a or b) for a,b in zip(ref1_comparison, ref2_comparison)] # For each question, 1 if extracted answer matches either answer1 or answer2, else 0\n","accuracy = sum(ref_comparison) / len(ref_comparison)\n","\n","print(\"ROUGE-L : \", round(100*rouge_score,2))\n","print(\"BLEU-1  : \", round(100*bleu1_score,2))\n","print(\"BLEU-4  : \", round(100*bleu4_score,2))\n","print(\"METEOR  : \", round(100*meteor_score,2))\n","print(\"CiDER   : \", round(100*cider_score,2))\n","print(\"Accuracy: \", round(100*accuracy, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tW7vfdCGvgND"},"source":[""],"execution_count":null,"outputs":[]}]}